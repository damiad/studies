{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZAD 4.1\n",
    "Aby zmienić rozmiar filtrów w dyskryminatorze na 4x4, należy zaktualizować funkcję Discriminator() i odpowiednio zmodyfikować warstwy konwolucyjne. Zmienimy rozmiar filtrów w warstwach downsample oraz Conv2D. Zmienione rozmiary filtrów wynoszą 4x4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    cloud_free_rgb = tf.keras.layers.Input(shape=[256, 256, 3], name='cloud_free_rgb')\n",
    "    edge_cloud_free_rgb = tf.keras.layers.Input(shape=[256, 256, 1], name='edge_cloud_free_rgb')\n",
    "    x = tf.keras.layers.concatenate([cloud_free_rgb, edge_cloud_free_rgb])\n",
    "\n",
    "    down1 = downsample(64, 4, apply_batchnorm=False)(x)  # Zmiana rozmiaru filtra na 4x4\n",
    "    down2 = downsample(128, 4)(down1)  # Zmiana rozmiaru filtra na 4x4\n",
    "    down3 = downsample(128, 4)(down2)  # Zmiana rozmiaru filtra na 4x4\n",
    "    down4 = downsample(256, 4)(down3)  # Zmiana rozmiaru filtra na 4x4\n",
    "    down5 = downsample(256, 4)(down4)  # Zmiana rozmiaru filtra na 4x4\n",
    "    down6 = downsample(512, 4)(down5)  # Zmiana rozmiaru filtra na 4x4\n",
    "    down7 = downsample(512, 4)(down6)  # Zmiana rozmiaru filtra na 4x4\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D(2)(down7) # Zmiana paddingu na 2\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,  # Zmiana rozmiaru filtra na 4x4\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  use_bias=False)(zero_pad1)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
    "    layer10 = tf.keras.layers.Conv2D(1, 4, strides=1,  # Zmiana rozmiaru filtra na 4x4\n",
    "                                     kernel_initializer=initializer)(zero_pad2)\n",
    "\n",
    "    flatten_layer = tf.keras.layers.Flatten()(layer10)\n",
    "    dense = tf.keras.layers.Dense(1)(flatten_layer)\n",
    "\n",
    "    return tf.keras.Model(inputs=[cloud_free_rgb, edge_cloud_free_rgb], outputs=tf.keras.activations.sigmoid(dense))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZAD 4.2\n",
    "Aby uwzględniać tylko obrazy RGB bez obrazów krawędzi, należy usunąć warstwę edge_cloud_free_rgb oraz odpowiednio dostosować wejście i wyjście modelu. Teraz wejściem dla dyskryminatora jest tylko obraz RGB bez zachmurzenia cloud_free_rgb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    cloud_free_rgb = tf.keras.layers.Input(shape=[256, 256, 3], name='cloud_free_rgb')\n",
    "    x = cloud_free_rgb\n",
    "    down1 = downsample(64, 3, False)(x)\n",
    "    down2 = downsample(128, 3)(down1)\n",
    "    down3 = downsample(128, 3)(down2)\n",
    "    down4 = downsample(256, 3)(down3)\n",
    "    down5 = downsample(256, 3)(down4)\n",
    "    down6 = downsample(512, 3)(down5)\n",
    "    down7 = downsample(512, 3)(down6)\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down7)\n",
    "    conv = tf.keras.layers.Conv2D(512, 3, strides=1,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  use_bias=False)(zero_pad1)\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
    "    layer10 = tf.keras.layers.Conv2D(1, 3, strides=1,\n",
    "                                     kernel_initializer=initializer)(zero_pad2)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(layer10)\n",
    "    dense = tf.keras.layers.Dense(1)(flatten_layer)\n",
    "    return tf.keras.Model(inputs=cloud_free_rgb, outputs=tf.keras.activations.sigmoid(dense))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZAD 4.3\n",
    "W zmodyfikowanym kodzie zostało usunięte \"skip-connections\". Wyjście z każdej warstwy downsample nie jest już przechowywane w liście skips, a konkatenacja z odpowiednim wyjściem z warstwy upsample została usunięta z pętli. Oto zmodyfikowany Generator():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    clouded_rgb = tf.keras.layers.Input(shape=[256, 256, 3], name='clouded_rgb')\n",
    "    clouded_nir = tf.keras.layers.Input(shape=[256, 256, 1], name='clouded_nir')\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),\n",
    "        downsample(128, 4),\n",
    "        downsample(256, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "    ]\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4),\n",
    "        upsample(256, 4),\n",
    "        upsample(128, 4),\n",
    "        upsample(64, 4),\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                           strides=2,\n",
    "                                           padding='same',\n",
    "                                           kernel_initializer=initializer,\n",
    "                                           activation='tanh')  # (bs, 256, 256, 3)\n",
    "    x = tf.keras.layers.concatenate([clouded_rgb, clouded_nir])\n",
    "    # Downsampling through the model\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "    # Upsampling\n",
    "    for up in up_stack:\n",
    "        x = up(x)\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=[clouded_rgb, clouded_nir], outputs=x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oba modele wytrenowałem na 100 epokach. Początkowo próbowałem na 500 epokach ale Google Collab przerywał mi za każdym razem około 120 epoki (przekroczenie RAMu w darmowej wersji).\n",
    "Obrazy wygenerowane przez pierwszy modelu ze \"skips connections\" są w katalogu: results_1.\n",
    "Obrazy wygenerowane przez drugi model z usuniętym \"skips connections\" są w katalogu: results_2.\n",
    "Skrpt wyświetlający to przejrzyście przedstawiam ponizej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model1_dir = 'results_1'\n",
    "model2_dir = 'results_2'\n",
    "\n",
    "num_images = 100\n",
    "\n",
    "plt.figure(figsize=(15, 200))\n",
    "\n",
    "for i in range(1, num_images + 1):\n",
    "    image_path_model1 = os.path.join(model1_dir, f'generated_cloud_free_rgb100_{i:03d}.png')\n",
    "    image_path_model2 = os.path.join(model2_dir, f'generated_cloud_free_rgb100_{i:03d}.png')\n",
    "\n",
    "    image_model1 = plt.imread(image_path_model1)\n",
    "    image_model2 = plt.imread(image_path_model2)\n",
    "\n",
    "    plt.subplot(num_images, 2, 2*i-1)\n",
    "    plt.imshow(image_model1)\n",
    "    plt.title(f'Model {1}, zdjęcie {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(num_images, 2, 2*i)\n",
    "    plt.imshow(image_model2)\n",
    "    plt.title(f'Model {2}, zdjęcie {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać na powyższych obrazach, obrazy wygenerowane przez model ze \"skip-connections\" są zdecydowanie bardziej realistyczne. Widać na nich rozróżnienie typów otoczenia (miasto, pole, las droga itp). Natomiast obrazy wygenerowane przez model bez \"skip-connections\" są \"wyciągnięte z kapelusza\". Od razu widać że model bez \"skip-connections\" generuje do każdego zdjęcia praktycznie idenytyczny obraz (coś w stylu gęsto zabudowanego miasta). Prowadzi to do wniosku, że model bez \"skip-connections\" nie nauczył się poprawnie rozpoznawać obrazów i praktycznie nie rozróżnia zdjęć między sobą. Na szczęscie model pierwszy ze \"skip-connections\" rozróżnia zdjęcia, uczy się i generuje coraz to dokładniejesze obrazy.\n",
    "Usunięcie skip connections z generatora prowadzi do poważnych negatywnych konsekwencji i uniemożliwia sukcesywne trenowanie modelu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
